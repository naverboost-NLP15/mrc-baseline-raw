
## roberta running
```
uv run train.py \
--output_dir ./models/train_dataset/roberta-large \
--do_train \
--do_eval \
--model_name_or_path klue/roberta-large \
--dataset_name ../data/train_dataset \
--per_device_train_batch_size 32 \
--per_device_eval_batch_size 32 \
--learning_rate 3e-5 \
--num_train_epochs 3 \
--weight_decay 0.01 \
--fp16 \
--logging_steps 100 \
--eval_steps 500 \
--save_steps 500 \
--evaluation_strategy steps \
--save_total_limit 2 \
--load_best_model_at_end \
```

```
python inference.py \
--output_dir ./predictions/ \
--dataset_name ../raw/data/test_dataset/ \
--model_name_or_path ./models/train_dataset/ \
--do_predict \
--eval_retrieval \
--top_k_retrieval 10
```
 

### TRAIN CODE
```
uv run code/src/train.py \
--output_dir ./models/train_dataset/baseline \
--do_train \
--do_eval \
--model_name_or_path klue/bert-base \
--dataset_name raw/data/train_dataset \
--per_device_train_batch_size 16 \
--per_device_eval_batch_size 16 \
--num_train_epochs 3 \
--fp16 \
--logging_steps 100 \
--eval_steps 500 \
--save_steps 500 \
--eval_strategy steps \
--save_total_limit 2 \
--load_best_model_at_end \
--metric_for_best_model exact_match \
--greater_is_better True \
--overwrite_output_dir
# --weight_decay 0.01 \
# --learning_rate 3e-5 \
```

### INFERENCE CODE
```
uv run code/inference.py \
--output_dir code/predictions/baseline_hybird_add_dense \
--dataset_name raw/data/test_dataset/ \
--model_name_or_path code/models/train_dataset/baseline_hybird_add_dense \
--do_predict \
--eval_retrieval \
--top_k_retrieval 10 \
--bm25_weight 0.5 \
--dense_weight 0.5 
```

uv run code/src/train.py \
--output_dir code/models/train_dataset/baseline \
--do_train \
--do_eval \
--model_name_or_path klue/bert-base \
--dataset_name raw/data/train_dataset \
--per_device_train_batch_size 16 \
--fp16 \
--num_train_epochs 3 \
--logging_steps 100 \
--eval_strategy steps \
--eval_steps 500 \
--save_steps 500 \
--overwrite_output_dir
--overwrite_cache

# Test data predictions

uv run code/src/inference.py \
--output_dir code/predictions/baseline_with_v3 \
--dataset_name raw/data/test_dataset/ \
--model_name_or_path code/models/train_dataset/baseline_with_v3 \
--do_predict \
--eval_retrieval \
--top_k_retrieval 5 \
--bm25_weight 0.5 \
--dense_weight 0.5 

# Vaildation data inference

uv run code/src/inference.py \
--output_dir code/predictions/baseline_real \
--dataset_name raw/data/train_dataset/ \
--model_name_or_path code/models/train_dataset/baseline \
--eval_retrieval \
--top_k_retrieval 5 \
--bm25_weight 0.5 \
--dense_weight 0.5 \
--do_eval


# baseline inference(Sparse Only)

uv run code/src/inference_sparse.py \
--output_dir code/predictions/baseline_eval \
--dataset_name raw/data/train_dataset/ \
--model_name_or_path code/models/train_dataset/baseline_with_v3 \
--eval_retrieval \
--top_k_retrieval 5 \
--do_eval



# roberta

uv run code/src/train.py \
--output_dir code/models/train_dataset/roberta_large \
--do_train \
--do_eval \
--model_name_or_path klue/roberta-large \
--dataset_name raw/data/train_dataset \
--per_device_train_batch_size 8 \
--per_device_eval_batch_size 8 \
--fp16 \
--num_train_epochs 3 \
--logging_steps 100 \
--eval_strategy steps \
--eval_steps 500 \
--save_steps 500 \
--save_total_limit 2 \
--load_best_model_at_end \
--metric_for_best_model exact_match \
--overwrite_output_dir \
--overwrite_cache
--learning_rate 3e-5 \
--weight_decay 0.01


# bert

uv run code/src/train.py \
--output_dir code/models/train_dataset/baseline_korquad \
--do_train \
--do_eval \
--model_name_or_path klue/bert-base \
--dataset_name raw/data/train_dataset \
--per_device_train_batch_size 8 \
--per_device_eval_batch_size 8 \
--fp16 \
--num_train_epochs 3 \
--logging_steps 100 \
--eval_strategy steps \
--eval_steps 500 \
--save_steps 500 \
--save_total_limit 2 \
--load_best_model_at_end \
--metric_for_best_model exact_match \
--overwrite_output_dir \
--overwrite_cache \
--learning_rate 3e-5 \
--weight_decay 0.01 \
--add_korquad True


# Korquad O, optim
uv run code/src/train.py \
--output_dir code/models/train_dataset/baseline_korquad_opt \
--do_train \
--do_eval \
--model_name_or_path klue/bert-base \
--dataset_name raw/data/train_dataset \
--add_korquad True \
\
--per_device_train_batch_size 16 \
--gradient_accumulation_steps 2 \
--per_device_eval_batch_size 32 \
\
--learning_rate 3e-5 \
--weight_decay 0.01 \
--num_train_epochs 3 \
--warmup_ratio 0.1 \
\
--logging_steps 500 \
--eval_strategy steps \
--eval_steps 500 \
--save_steps 500 \
--save_total_limit 2 \
\
--load_best_model_at_end \
--metric_for_best_model exact_match \
--overwrite_output_dir \
--fp16 \
--seed 42

# Korquad X, optim
uv run code/src/train.py \
--output_dir code/models/train_dataset/baseline_no_korquad \
--do_train \
--do_eval \
--model_name_or_path klue/bert-base \
--dataset_name raw/data/train_dataset \
--add_korquad False \
--per_device_train_batch_size 16 \
--gradient_accumulation_steps 2 \
--per_device_eval_batch_size 32 \
--learning_rate 3e-5 \
--weight_decay 0.01 \
--num_train_epochs 10 \
--warmup_ratio 0.1 \
--logging_steps 100 \
--eval_strategy steps \
--eval_steps 100 \
--save_steps 100 \
--save_total_limit 2 \
--load_best_model_at_end \
--metric_for_best_model exact_match \
--overwrite_output_dir \
--fp16 \
--seed 42


# Korquad O, roberta-large

uv run code/src/train.py \
--output_dir code/models/train_dataset/roberta_large_korquad \
--do_train \
--do_eval \
--model_name_or_path klue/roberta-large \
--dataset_name raw/data/train_dataset \
--add_korquad True \
\
--per_device_train_batch_size 4 \
--gradient_accumulation_steps 8 \
--per_device_eval_batch_size 32 \
\
--learning_rate 1.5e-5 \
--weight_decay 0.01 \
--num_train_epochs 3 \
--warmup_ratio 0.1 \
\
--logging_steps 500 \
--eval_strategy steps \
--eval_steps 500 \
--save_steps 500 \
--save_total_limit 2 \
\
--load_best_model_at_end \
--metric_for_best_model exact_match \
--overwrite_output_dir \
--fp16 \
--seed 42

# Korquad X, roberta-large

uv run code/src/train.py \
--output_dir code/models/train_dataset/roberta_large_no_korquad \
--do_train \
--do_eval \
--model_name_or_path klue/roberta-large \
--dataset_name raw/data/train_dataset \
--add_korquad False \
\
--per_device_train_batch_size 4 \
--gradient_accumulation_steps 8 \
--per_device_eval_batch_size 32 \
\
--learning_rate 1.5e-5 \
--weight_decay 0.01 \
--num_train_epochs 10 \
--warmup_ratio 0.1 \
\
--logging_steps 100 \
--eval_strategy steps \
--eval_steps 100 \
--save_steps 100 \
--save_total_limit 2 \
\
--load_best_model_at_end \
--metric_for_best_model exact_match \
--overwrite_output_dir \
--fp16 \
--seed 42


### "원본 + korquad" 후 "원본" 추가 파인튜닝
uv run code/src/train.py \
--output_dir code/models/train_dataset/roberta_large_finetuned_final \
--do_train \
--do_eval \
--model_name_or_path ./code/models/train_dataset/roberta_large_korquad \
--dataset_name raw/data/train_dataset \
--add_korquad False \
\
--per_device_train_batch_size 16 \
--gradient_accumulation_steps 2 \
--per_device_eval_batch_size 32 \
\
--learning_rate 1e-5 \
--weight_decay 0.01 \
--num_train_epochs 5 \
--warmup_ratio 0.1 \
\
--logging_steps 100 \
--eval_strategy steps \
--eval_steps 100 \
--save_steps 100 \
--save_total_limit 2 \
\
--load_best_model_at_end \
--metric_for_best_model exact_match \
--overwrite_output_dir \
--fp16 \
--seed 42
(참고: LR은 `1e-5`로 낮춰서 조심스럽게 다듬는 것이 포인트입니다.)